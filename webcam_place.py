import os
import sys
import cv2
import json
import math
import time
import threading
import pyttsx3
import mediapipe as mp
from ultralytics import YOLO
from datetime import datetime

# ì„¤ì •
TASK_FILENAME = "task_plan_destination.json"
ARRIVE_THRESHOLD = 50
NEAR_THRESHOLD = 150
DISTANCE_DELTA = 30
FEEDBACK_INTERVAL = 2.0
YOLO_INTERVAL = 1.5
TTS_RATE = 200
MIN_FEEDBACK_INTERVAL = 4
GRAB_HOLD_DURATION = 3.0
HAND_FEEDBACK_INTERVAL = 9.0

# ì´ˆê¸°í™”
model = YOLO("yolov8n.pt")
tts = pyttsx3.init()
tts.setProperty("rate", TTS_RATE)
tts_lock = threading.Lock()
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1,
                       min_detection_confidence=0.7, min_tracking_confidence=0.7)

# ì „ì—­ ë³€ìˆ˜
target, destination = None, None
target_pos, destination_pos = None, None
last_seen_target_pos, last_seen_destination_pos = None, None
hand_pos, frame_for_display = None, None
frame_lock = threading.Lock()
last_hand_feedback_time = 0
near_intro_done = False

# í”¼ë“œë°± ìƒíƒœ
last_feedback_time, last_feedback_text = 0, None
current_tts_thread = None
step = "find_target"
prev_distance = None
target_intro_done, destination_intro_done = False, False
target_grabbed = False
last_close_to_target_time = None
initial_target_direction_given = False

def speak_feedback(text):
    global last_feedback_time, last_feedback_text, current_tts_thread
    now = time.time()
    if text == last_feedback_text and now - last_feedback_time < MIN_FEEDBACK_INTERVAL:
        return
    last_feedback_text = text
    last_feedback_time = now
    print("ğŸ—£", text)

    def tts_job():
        with tts_lock:
            try:
                tts.stop()
                tts.say(text)
                tts.runAndWait()
            except:
                pass

    if current_tts_thread and current_tts_thread.is_alive():
        pass
    current_tts_thread = threading.Thread(target=tts_job, daemon=True)
    current_tts_thread.start()

def speak_hand_feedback(text):
    global last_hand_feedback_time, current_tts_thread
    now = time.time()
    if now - last_hand_feedback_time < HAND_FEEDBACK_INTERVAL:
        return
    last_hand_feedback_time = now
    print("ğŸ—£", text)

    def tts_job():
        with tts_lock:
            try:
                tts.stop()
                tts.say(text)
                tts.runAndWait()
            except:
                pass

    if current_tts_thread and current_tts_thread.is_alive():
        current_tts_thread.join()
    current_tts_thread = threading.Thread(target=tts_job, daemon=True)
    current_tts_thread.start()

def detect_hand(image):
    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb)
    if results.multi_hand_landmarks:
        h, w, _ = image.shape
        palm = [results.multi_hand_landmarks[0].landmark[i] for i in [0, 1, 5, 9, 13, 17]]
        x = int(sum(p.x for p in palm) / len(palm) * w)
        y = int(sum(p.y for p in palm) / len(palm) * h)
        return (x, y)
    return None

def find_object_position(image, label, min_conf=0.6, bottom_only=False):
    resized = cv2.resize(image, (320, 320))
    results = model(resized, verbose=False)
    scale_x = image.shape[1] / 320
    scale_y = image.shape[0] / 320
    for result in results:
        for box in result.boxes:
            cls = model.names[int(box.cls)].lower()
            conf = float(box.conf[0])
            if label.lower() in cls and conf > min_conf:
                x1, y1, x2, y2 = box.xyxy[0].tolist()
                cx = int((x1 + x2) / 2 * scale_x)
                if bottom_only:
                    cy = int(y2 * scale_y)  # ë°”ë‹¥ ë¶€ë¶„
                else:
                    cy = int((y1 + y2) / 2 * scale_y)  # ì¤‘ì•™ ë¶€ë¶„
                return (cx, cy)
    return None

def get_initial_direction_comment(pos, frame_size):
    if pos is None:
        return "íƒ€ê²Ÿ ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    x, y = pos
    w, h = frame_size
    x_rel, y_rel = x / w, y / h
    dir_x = "ì™¼ìª½" if x_rel < 0.3 else "ì˜¤ë¥¸ìª½" if x_rel > 0.7 else "ê°€ìš´ë°"
    dir_y = "ìœ„" if y_rel < 0.3 else "ì•„ë˜" if y_rel > 0.7 else "ê°€ìš´ë°"
    if dir_x == "ê°€ìš´ë°" and dir_y == "ê°€ìš´ë°":
        return "íƒ€ê²Ÿì´ ì •ì¤‘ì•™ì— ìˆìŠµë‹ˆë‹¤."
    else:
        return f"íƒ€ê²Ÿì´ {dir_x} {dir_y}ì— ìˆì–´ìš”."

def feedback_loop():
    global step, target_intro_done, destination_intro_done
    global prev_distance, target_grabbed, last_close_to_target_time
    global initial_target_direction_given, last_hand_feedback_time
    global near_intro_done

    while True:
        time.sleep(FEEDBACK_INTERVAL)

        with frame_lock:
            hand = hand_pos
            target = target_pos or last_seen_target_pos
            dest = destination_pos or last_seen_destination_pos
            frame = frame_for_display.copy() if frame_for_display is not None else None

        if step == "find_target":
            if frame is None:
                continue
            if not target and not destination:
                speak_feedback("íƒ€ê²Ÿê³¼ ëª©ì ì§€ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue
            elif not target:
                speak_feedback("íƒ€ê²Ÿì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue
            elif not destination:
                speak_feedback("ëª©ì ì§€ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue

            if not initial_target_direction_given and target and isinstance(target, tuple):
                msg = get_initial_direction_comment(target, (frame.shape[1], frame.shape[0]))
                speak_feedback(msg)
                initial_target_direction_given = True

            if not hand:
                now = time.time()
                if now - last_hand_feedback_time > HAND_FEEDBACK_INTERVAL:
                    speak_hand_feedback("ì†ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    last_hand_feedback_time = now
                continue

            hx, hy = hand
            tx, ty = target
            dx, dy = tx - hx, ty - hy
            distance = math.sqrt(dx ** 2 + dy ** 2)

            if distance < ARRIVE_THRESHOLD:
                if not last_close_to_target_time:
                    last_close_to_target_time = datetime.now()
                elif (datetime.now() - last_close_to_target_time).total_seconds() >= GRAB_HOLD_DURATION:
                    speak_feedback("ë„ì°©í–ˆìŠµë‹ˆë‹¤. ì†ì„ ë»—ì–´ ì¡ìœ¼ì„¸ìš”.")
                    target_grabbed = True
                    step = "move_to_destination"
                    destination_intro_done = False
                    target_intro_done = False
                    time.sleep(1.5)
                    continue
            else:
                last_close_to_target_time = None

                if abs(dx) < 30 and abs(dy) < 30:
                    continue  # 30ë³´ë‹¤ ê°€ê¹Œìš°ë©´ ë°©í–¥ í”¼ë“œë°± ìƒëµ

            direction = "ì˜¤ë¥¸ìª½" if dx > 0 else "ì™¼ìª½" if abs(dx) > abs(dy) else "ì•„ë˜" if dy > 0 else "ìœ„"

            if distance < NEAR_THRESHOLD:
                if not near_intro_done:
                    speak_feedback(f"ê±°ì˜ ë„ì°©í–ˆì–´ìš”. {direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
                    near_intro_done = True
                else:
                    speak_feedback(f"{direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
            else:
                near_intro_done = False
                if not target_intro_done:
                    speak_feedback(f"íƒ€ê²Ÿì— ì ‘ê·¼ ì¤‘ì…ë‹ˆë‹¤. {direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
                    target_intro_done = True
                else:
                    speak_feedback(f"{direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")

        elif step == "move_to_destination":
            if frame is None:
                continue
            if not dest:
                speak_feedback("ëª©ì ì§€ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue
            if not hand:
                now = time.time()
                if now - last_hand_feedback_time > HAND_FEEDBACK_INTERVAL:
                    speak_hand_feedback("ì†ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    last_hand_feedback_time = now
                continue

            hx, hy = hand
            dx, dy = dest[0] - hx, dest[1] - hy
            distance = math.sqrt(dx ** 2 + dy ** 2)

            if distance < ARRIVE_THRESHOLD:
                speak_feedback("ëª©ì ì§€ì— ë„ì°©í–ˆìŠµë‹ˆë‹¤. ë‚´ë ¤ë†“ìœ¼ì„¸ìš”.")
                step = "done"
                continue

            direction = "ì˜¤ë¥¸ìª½" if dx > 0 else "ì™¼ìª½" if abs(dx) > abs(dy) else "ì•„ë˜" if dy > 0 else "ìœ„"

            if not destination_intro_done:
                speak_feedback("ëª©ì ì§€ë¡œ ì´ë™ ì¤‘ì…ë‹ˆë‹¤.")
                destination_intro_done = True
            else:
                speak_feedback(f"{direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")

def yolo_loop():
    global target_pos, destination_pos, last_seen_target_pos, last_seen_destination_pos
    while True:
        time.sleep(YOLO_INTERVAL)
        with frame_lock:
            frame = frame_for_display.copy() if frame_for_display is not None else None
        if frame is None:
            continue

        # íƒ€ê²Ÿì€ ì¤‘ì•™ ê¸°ì¤€
        if target:
            pos = find_object_position(frame, target, bottom_only=False)
            if pos:
                target_pos = pos
                last_seen_target_pos = pos

        # ëª©ì ì§€ëŠ” í•˜ë‹¨ ê¸°ì¤€
        if destination:
            pos = find_object_position(frame, destination, bottom_only=True)
            if pos:
                destination_pos = pos
                last_seen_destination_pos = pos


def load_target_info():
    try:
        with open(TASK_FILENAME, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get("target"), data.get("destination")
    except:
        return None, None

if __name__ == "__main__":
    target, destination = load_target_info()
    if not target or not destination:
        print("âŒ íƒ€ê²Ÿ ë˜ëŠ” ëª©ì ì§€ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        exit()
    print(f"ğŸ“Œ íƒ€ê²Ÿ: {target}, ëª©ì ì§€: {destination}")

    cap = cv2.VideoCapture(1, cv2.CAP_DSHOW)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    threading.Thread(target=feedback_loop, daemon=True).start()
    threading.Thread(target=yolo_loop, daemon=True).start()

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        with frame_lock:
            frame_for_display = frame.copy()
            if any([target_pos, destination_pos, last_seen_target_pos, last_seen_destination_pos]):
                hand_pos = detect_hand(frame)
            else:
                hand_pos = None

        if hand_pos:
            cv2.circle(frame_for_display, hand_pos, 10, (0, 255, 0), -1)
        tp = target_pos or last_seen_target_pos
        if tp and isinstance(tp, tuple):
            cv2.circle(frame_for_display, tp, 10, (0, 0, 255), -1)
        dp = destination_pos or last_seen_destination_pos
        if dp and isinstance(dp, tuple):
            cv2.circle(frame_for_display, dp, 10, (255, 0, 0), -1)

        cv2.imshow("ì›¹ìº  ë¯¸ë¦¬ë³´ê¸°", frame_for_display)
        if cv2.waitKey(1) & 0xFF == 27:
            break

    cap.release()
    cv2.destroyAllWindows()
