import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"   # 0=ALL, 1=INFO, 2=WARNING, 3=ERROR
import logging
logging.getLogger("absl").setLevel(logging.ERROR)
import sys
import cv2
if hasattr(cv2, 'utils') and hasattr(cv2.utils, 'logging'):
    cv2.utils.logging.setLogLevel(cv2.utils.logging.LOG_LEVEL_SILENT)
import json
import math
import time
import threading
import pyttsx3
import mediapipe as mp
try:
    mp.logging.set_verbosity(mp.logging.ERROR)
except:
    # older versions of MediaPipe
    logging.getLogger('mediapipe').setLevel(logging.ERROR)
from ultralytics import YOLO
from ultralytics import YOLO as _YOLO
from datetime import datetime
import openai
import base64
from voice_command import get_yes_no_response, get_command
from task_planner_dest import load_command, extract_target_with_gpt

# ì„¤ì •
TASK_FILENAME = "task_plan_destination.json"
ARRIVE_THRESHOLD = 50
NEAR_THRESHOLD = 150
DISTANCE_DELTA = 30
FEEDBACK_INTERVAL = 2.0
YOLO_INTERVAL = 1.5
TTS_RATE = 220
MIN_FEEDBACK_INTERVAL = 5.0
HAND_FEEDBACK_INTERVAL = 7.0
api_key = os.getenv("OPENAI_API_KEY")

# ì´ˆê¸°í™”
model = YOLO("yolov8n.pt")
tts = pyttsx3.init()
tts.setProperty("rate", TTS_RATE)
tts_lock = threading.Lock()
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1,
                       min_detection_confidence=0.7, min_tracking_confidence=0.7)

# ì „ì—­ ë³€ìˆ˜
target, destination = None, None
target_pos, destination_pos = None, None
last_seen_target_pos, last_seen_destination_pos = None, None
last_seen_target_time = 0.0
last_seen_destination_time = 0.0
EXPIRE_TIME = 3.0   #3ì´ˆì´ìƒ ê°ì§€ ì•ˆ ë˜ë©´ ì  ì§€ì›€
hand_pos, frame_for_display = None, None
frame_lock = threading.Lock()
last_hand_feedback_time = 0
near_intro_done = False
miss_count = 0  # ë¬¼ì²´ ë¯¸ê²€ì¶œ ì‹œ ì¹´ìš´íŠ¸
pano_width, pano_height = None, None
pan_target_pos, pan_dest_pos = None, None
pan_target_prompted = False
pan_dest_prompted = False

# í”¼ë“œë°± ìƒíƒœ
last_feedback_time, last_feedback_text = 0, None
current_tts_thread = None
step = "find_target"
prev_distance = None
target_intro_done, destination_intro_done = False, False
target_grabbed = False
last_close_to_target_time = None
initial_target_direction_given = False



def auto_panorama_scan(scan_duration=7.0, capture_interval=0.6, cam_index=2):
    cap = cv2.VideoCapture(cam_index, cv2.CAP_DSHOW)
    frames = []
    print("ğŸ—£ íŒŒë…¸ë¼ë§ˆ ìŠ¤ìº”ì„ 7ì´ˆë™ì•ˆ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì²œì²œíˆ ëŒì•„ë´ì£¼ì„¸ìš”.")
    tts.say("íŒŒë…¸ë¼ë§ˆ ìŠ¤ìº”ì„ 7ì´ˆë™ì•ˆ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤. ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì²œì²œíˆ ëŒì•„ë´ì£¼ì„¸ìš”.")
    tts.runAndWait()

    start_t = time.time()
    next_capture = start_t

    while time.time() - start_t < scan_duration:
        ret, frame = cap.read()
        if not ret:
            break
        now = time.time()
        if now >= next_capture:
            frames.append(frame.copy())
            next_capture += capture_interval
        cv2.waitKey(1)
    cap.release()
    # stitch
    stitcher = cv2.Stitcher_create()
    status, pano = stitcher.stitch(frames)
    if status != cv2.Stitcher_OK:
        print("âš ï¸ íŒŒë…¸ë¼ë§ˆ ìƒì„± ì‹¤íŒ¨:", status)
        return None
    print("âœ… íŒŒë…¸ë¼ë§ˆ ìƒì„± ì™„ë£Œ")
    return pano

def detect_on_panorama(pano_img, target_label, dest_label=None, return_labels=False):
    model = _YOLO("yolov8n.pt")
    results = model(pano_img)
    tgt, dst = None, None
    labels = []
    for r in results:
        for box in r.boxes:
            cls = model.names[int(box.cls)].lower()
            labels.append(cls)

            x1, y1, x2, y2 = box.xyxy[0].tolist()
            cx = int((x1 + x2)/2)
            cy = int((y1 + y2)/2)
            if target_label.lower() in cls:
                tgt = (cx, cy)
            if dest_label and dest_label.lower() in cls:
                dst = (cx, cy)
    if return_labels:
        return tgt, dst, labels
    return tgt, dst


def speak_feedback(text):
    global last_feedback_time, last_feedback_text, current_tts_thread
    now = time.time()
    if text == last_feedback_text and now - last_feedback_time < MIN_FEEDBACK_INTERVAL:
        return
    last_feedback_text = text
    last_feedback_time = now
    print("ğŸ—£", text)

    # def tts_job():
    #     with tts_lock:
    #         try:
    #             tts.stop()
    #             tts.say(text)
    #             tts.runAndWait()
    #         except:
    #             pass
    #
    # if current_tts_thread and current_tts_thread.is_alive():
    #     pass
    # current_tts_thread = threading.Thread(target=tts_job, daemon=True)
    # current_tts_thread.start()

    with tts_lock:
        tts.say(text)
        tts.runAndWait()

def speak_hand_feedback(text):
    global last_hand_feedback_time, current_tts_thread
    now = time.time()
    if now - last_hand_feedback_time < HAND_FEEDBACK_INTERVAL:
        return
    last_hand_feedback_time = now
    print("ğŸ—£", text)

    # def tts_job():
    #     with tts_lock:
    #         try:
    #             tts.stop()
    #             tts.say(text)
    #             tts.runAndWait()
    #         except:
    #             pass
    #
    # if current_tts_thread and current_tts_thread.is_alive():
    #     pass
    # current_tts_thread = threading.Thread(target=tts_job, daemon=True)
    # current_tts_thread.start()

    with tts_lock:
        tts.say(text)
        tts.runAndWait()

def detect_hand(image):
    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb)
    if results.multi_hand_landmarks:
        h, w, _ = image.shape
        palm = [results.multi_hand_landmarks[0].landmark[i] for i in [0, 1, 5, 9, 13, 17]]
        x = int(sum(p.x for p in palm) / len(palm) * w)
        y = int(sum(p.y for p in palm) / len(palm) * h)
        return (x, y)
    return None

def find_object_position(image, label, min_conf=0.6):
    resized = cv2.resize(image, (320, 320))
    results = model(resized, verbose=False)
    scale_x = image.shape[1] / 320
    scale_y = image.shape[0] / 320
    for result in results:
        for box in result.boxes:
            cls = model.names[int(box.cls)].lower()
            conf = float(box.conf[0])
            if label.lower() in cls and conf > min_conf:
                x1, y1, x2, y2 = box.xyxy[0].tolist()
                cx = int((x1 + x2) / 2 * scale_x)
                cy = int(y2 * scale_y)  # í•˜ë‹¨ ë¶€ë¶„
                return (cx, cy)
    return None

def get_initial_direction_comment(pos, frame_size):
    # if pos is None:
    #     return "íƒ€ê²Ÿ ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    x, y = pos
    w, h = frame_size
    x_rel, y_rel = x / w, y / h
    dir_x = "ì™¼ìª½" if x_rel < 0.3 else "ì˜¤ë¥¸ìª½" if x_rel > 0.7 else "ê°€ìš´ë°"
    dir_y = "ìœ„" if y_rel < 0.3 else "ì•„ë˜" if y_rel > 0.7 else "ê°€ìš´ë°"
    if dir_x == "ê°€ìš´ë°" and dir_y == "ê°€ìš´ë°":
        return "íƒ€ê²Ÿì´ ì •ì¤‘ì•™ì— ìˆìŠµë‹ˆë‹¤."
    else:
        return f"íƒ€ê²Ÿì´ {dir_x} {dir_y}ì— ìˆì–´ìš”."

def ask_gpt_if_grabbed(image, target):
    question = (
        f"ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì‚¬ëŒì´ '{target}'ë¥¼ ì†ìœ¼ë¡œ í™•ì‹¤íˆ ì¡ê³  ìˆëŠ”ì§€ íŒë‹¨í•´ì¤˜. "
        "ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì—ë§Œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´:\n"
        '{ "grabbed": "true" }\n\n'
        "- ì†ì´ ë¬¼ì²´ë¥¼ ê°ì‹¸ê³  ìˆê±°ë‚˜\n"
        "- ì†ê°€ë½ì´ ë¬¼ì²´ì˜ ì–‘ìª½ì„ ì¡ê³  ìˆìœ¼ë©°\n"
        "- ì†ì´ ë¬¼ì²´ ìœ„ì— ë†“ì—¬ í™•ì‹¤íˆ ê³ ì •ëœ ìƒíƒœë¼ë©´ true.\n\n"
        "ì†ì´ ê·¼ì²˜ì— ìˆê±°ë‚˜ ì†ê°€ë½ì´ ë¬¼ì²´ ìœ„ì— ì—†ìœ¼ë©´ falseë¡œ íŒë‹¨í•´ì¤˜:\n"
        '{ "grabbed": "false" }\n\n'
        "ë°˜ë“œì‹œ ìœ„ JSON í˜•ì‹ ì¤‘ í•˜ë‚˜ë¡œë§Œ ëŒ€ë‹µí•´ì¤˜."
    )

    _, buffer = cv2.imencode('.jpg', image)
    img_bytes = buffer.tobytes()
    base64_img = base64.b64encode(img_bytes).decode("utf-8")

    print("ğŸ§  GPT íŒë‹¨ ìš”ì²­ ì¤‘...")

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": question},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{base64_img}"}
                        }
                    ]
                }
            ],
            max_tokens=300,
        )

        result = response['choices'][0]['message']['content']
        print("ğŸ§  GPT ì‘ë‹µ:", result)

        # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„
        try:
            parsed = json.loads(result)
            return str(parsed.get("grabbed", "")).strip().lower() == "true"
        except json.JSONDecodeError:
            print("âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨. ì‘ë‹µ ë‚´ìš© í™•ì¸:", result)
            return False

    except Exception as e:
        print("[GPT ì˜¤ë¥˜]", e)
        return False


# ------------------ í”¼ë“œë°± ë£¨í”„ ------------------

def feedback_loop():
    global step, target_intro_done, destination_intro_done, target_grabbed, last_close_to_target_time
    global initial_target_direction_given, last_hand_feedback_time
    global pan_target_prompted, pan_dest_prompted

    while True:
        time.sleep(FEEDBACK_INTERVAL)

        with frame_lock:
            hand = hand_pos
            tgt = target_pos or last_seen_target_pos
            dst = destination_pos or last_seen_destination_pos
            frame = frame_for_display.copy() if frame_for_display is not None else None

        # 1) find_target ë‹¨ê³„
        if step == "find_target":
            # íƒ€ê²Ÿì´ ì²˜ìŒ ê°ì§€ë˜ì—ˆì„ ë•Œ, ë”± í•œ ë²ˆë§Œ ë°©í–¥ ì•ˆë‚´
            if tgt is not None and not initial_target_direction_given:
                # frame í¬ê¸°: (width, height)
                msg = get_initial_direction_comment(tgt, (frame.shape[1], frame.shape[0]))
                speak_feedback(msg)
                initial_target_direction_given = True
            # íƒ€ê²Ÿì´ ì—†ìœ¼ë©´ íŒŒë…¸ë¼ë§ˆ ê¸°ì¤€ ì¢Œ/ìš° ì•ˆë‚´ (í•œ ë²ˆë§Œ)
            if tgt is None:
                if pan_target_pos:
                    if not pan_target_prompted:
                        msg = "ì˜¤ë¥¸ìª½ì„ ë´ì£¼ì„¸ìš”." if pan_target_pos[0] > pano_width / 2 else "ì™¼ìª½ì„ ë´ì£¼ì„¸ìš”."
                        speak_feedback(msg)
                        pan_target_prompted = True
                else:
                    speak_feedback("íƒ€ê²Ÿ ìœ„ì¹˜ ì •ë³´ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue

            # íƒ€ê²Ÿì´ ë³´ì´ë©´ í”Œë˜ê·¸ ë¦¬ì…‹
            pan_target_prompted = False

            # íƒ€ê²Ÿì´ ì¡í˜”ëŠ”ì§€ í™•ì¸í•˜ê¸° ì „ê¹Œì§€ëŠ” ê±°ë¦¬ ì•ˆë‚´
            if not hand:
                speak_hand_feedback("ì†ì´ ê°ì§€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                continue

            dx, dy = tgt[0] - hand[0], tgt[1] - hand[1]
            dist = math.hypot(dx, dy)

            # ì†ì´ íƒ€ê²Ÿ ê°€ê¹Œì´ ì™”ì„ ë•Œ ì¡ê¸° ìœ ë„ + í™•ì¸
            if dist < ARRIVE_THRESHOLD:
                if not last_close_to_target_time:
                    last_close_to_target_time = datetime.now()
                elif (datetime.now() - last_close_to_target_time).total_seconds() >= 3:
                    speak_feedback("ì†ì„ ë»—ì–´ ì¡ìœ¼ì„¸ìš”.")
                    # ì¡ê¸° í™•ì¸ ë¡œì§
                    time.sleep(4.0)
                    with frame_lock:
                        latest_frame = frame_for_display.copy() if frame_for_display is not None else None

                    if latest_frame is not None and hand is not None:
                        hx, hy = hand
                        tx, ty = tgt
                        h, w, _ = latest_frame.shape
                        mx, my = int((hx + tx) / 2), int((hy + ty) / 2)
                        x1, x2 = max(0, mx - 200), min(w, mx + 200)
                        y1, y2 = max(0, my - 200), min(h, my + 200)
                        crop = latest_frame[y1:y2, x1:x2]
                        cv2.imwrite("debug_crop.jpg", crop)

                        is_grabbed = ask_gpt_if_grabbed(crop, target)
                        if isinstance(is_grabbed, str):
                            is_grabbed = is_grabbed.strip().lower() == "true"

                        if is_grabbed:
                            speak_feedback("ì˜ ì¡ì•˜ì–´ìš”.")
                            target_grabbed = True
                            step = "move_to_destination"
                            target_intro_done = False
                            destination_intro_done = False
                            time.sleep(1.5)
                            continue
                        else:
                            speak_feedback("ì•„ì§ ì¡ì§€ ì•Šì€ ê²ƒ ê°™ì•„ìš”.")
            else:
                last_close_to_target_time = None
                # ì†â†”íƒ€ê²Ÿ ê±°ë¦¬ ì•ˆë‚´
                direction = "ì˜¤ë¥¸ìª½" if dx > 0 else "ì™¼ìª½" if abs(dx) > abs(dy) else "ìœ„" if dy < 0 else "ì•„ë˜"
                speak_feedback(f"{direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
            continue


        # 2) move_to_destination ë‹¨ê³„
        elif step == "move_to_destination":
            # (a) ëª©ì ì§€ê°€ í™”ë©´ì— ë³´ì´ì§€ ì•Šì„ ë•Œ â†’ íŒŒë…¸ë¼ë§ˆ ê¸°ì¤€ ì¢Œ/ìš° ì•ˆë‚´
            if dst is None:
                if pan_dest_pos:
                    if not pan_dest_prompted:
                        msg = "ì˜¤ë¥¸ìª½ì„ ë´ì£¼ì„¸ìš”." if pan_dest_pos[0] > pano_width / 2 else "ì™¼ìª½ì„ ë´ì£¼ì„¸ìš”."
                        speak_feedback(msg)
                        pan_dest_prompted = True
                else:
                    speak_feedback("ëª©ì ì§€ ìœ„ì¹˜ ì •ë³´ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue

            pan_dest_prompted = False

            # (b) ëª©ì ì§€ê°€ ë³´ì´ë©´ ì†â†”ëª©ì ì§€ ê±°ë¦¬ ì•ˆë‚´
            if not hand:
                speak_hand_feedback("ì†ì´ ê°ì§€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                continue

            dx, dy = dst[0] - hand[0], dst[1] - hand[1]
            dist = math.hypot(dx, dy)

            if dist < ARRIVE_THRESHOLD:
                speak_feedback("ëª©ì ì§€ì— ë„ì°©í–ˆìŠµë‹ˆë‹¤. ë‚´ë ¤ë†“ìœ¼ì„¸ìš”.")
                step = "done"
            else:
                direction = "ì˜¤ë¥¸ìª½" if dx > 0 else "ì™¼ìª½" if abs(dx) > abs(dy) else "ìœ„" if dy < 0 else "ì•„ë˜"
                speak_feedback(f"{direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
            continue


# ------------------ ê°ì²´ ìœ„ì¹˜ ê°±ì‹  ë£¨í”„ ------------------

def yolo_loop():
    global target_pos, destination_pos, last_seen_target_pos, last_seen_destination_pos
    global last_seen_target_time, last_seen_destination_time
    while True:
        time.sleep(YOLO_INTERVAL)
        with frame_lock:
            frame = frame_for_display.copy() if frame_for_display is not None else None
        if frame is None:
            continue

        if target:
            pos = find_object_position(frame, target)
            if pos:
                target_pos = last_seen_target_pos = pos
                last_seen_target_time = time.time()
        if destination:
            pos = find_object_position(frame, destination)
            if pos:
                destination_pos = last_seen_destination_pos = pos
                last_seen_destination_time = time.time()


# ------------------ ë©”ì¸ ------------------

def load_target_info():
    try:
        with open(TASK_FILENAME, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get("target"), data.get("destination")
    except:
        return None, None



if __name__ == "__main__":
    target, destination = load_target_info()
    if not target or not destination:
        print("âŒ íƒ€ê²Ÿ ë˜ëŠ” ëª©ì ì§€ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        exit()
    print(f"ğŸ“Œ íƒ€ê²Ÿ: {target}, ëª©ì ì§€: {destination}")

    # 1) íŒŒë…¸ë¼ë§ˆ ìŠ¤ìº” ë° ê²€ì¶œ
    print("ğŸ—£ íŒŒë…¸ë¼ë§ˆ ìŠ¤ìº”ì„ ìœ„í•´ 10ì‹œ ë°©í–¥ìœ¼ë¡œ ëª¸ì„ ëŒë ¤ì£¼ì„¸ìš”.")
    tts.say("íŒŒë…¸ë¼ë§ˆ ìŠ¤ìº”ì„ ìœ„í•´ 10ì‹œ ë°©í–¥ìœ¼ë¡œ ëª¸ì„ ëŒë ¤ì£¼ì„¸ìš”.")
    tts.runAndWait()
    pano = auto_panorama_scan(scan_duration=7.0, capture_interval=0.6, cam_index=2)

    if pano is None:
        # stitch ì‹¤íŒ¨ ì‹œ í•œ ë²ˆ ë” ì¬ì‹œë„
        speak_feedback("íŒŒë…¸ë¼ë§ˆ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œ ë²ˆ ìŠ¤ìº”í•˜ê² ìŠµë‹ˆë‹¤.")
        speak_feedback("8ì‹œ ë°©í–¥ìœ¼ë¡œ ëŒì•„ì£¼ì„¸ìš”.")
        pano = auto_panorama_scan(scan_duration=7.0, capture_interval=0.6, cam_index=2)

    if pano is None:
        sys.exit("íŒŒë…¸ë¼ë§ˆ ì¬ì‹œë„ì—ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

    cv2.imwrite("panorama.jpg", pano)
    tgt_pos, dst_pos, labels = detect_on_panorama(pano, target, destination, return_labels=True)
    pano_width, pano_height = pano.shape[1], pano.shape[0]
    pan_target_pos, pan_dest_pos = tgt_pos, dst_pos

    # ë‘˜ ë‹¤ ëª» ì°¾ì•˜ìœ¼ë©´ í•œ ë²ˆ ë” ì‹œë„
    if not tgt_pos and not dst_pos:
        speak_feedback("íŒŒë…¸ë¼ë§ˆì—ì„œ íƒ€ê²Ÿê³¼ ëª©ì ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ í•œ ë²ˆ ìŠ¤ìº”í•˜ê² ìŠµë‹ˆë‹¤.")
        speak_feedback("8ì‹œ ë°©í–¥ìœ¼ë¡œ ëŒì•„ì£¼ì„¸ìš”.")
        pano = auto_panorama_scan(scan_duration=7.0, capture_interval=0.6, cam_index=2)
        if pano is None:
            sys.exit("íŒŒë…¸ë¼ë§ˆ ì¬ìŠ¤ìº” ì‹¤íŒ¨ â€” í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        cv2.imwrite("panorama.jpg", pano)
        if destination:
            tgt_pos, dst_pos, labels = detect_on_panorama(pano, target, destination, return_labels=True)
        else:
            tgt_pos, _, labels = detect_on_panorama(pano, target, None, return_labels=True)
            dst_pos = None

    # ë‘ ë²ˆ ì‹œë„ í›„ì—ë„ ëª» ì°¾ì•˜ì„ ë•Œ
    if not tgt_pos and not dst_pos:
        seen = ", ".join(sorted(set(labels))) or "ì—†ìŒ"
        speak_feedback(
            f"ì°¾ìœ¼ì‹œëŠ” {target}"
            + (f" / {destination}" if destination else "")
            + f" ë¬¼ì²´ëŠ” ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤. "
            f"í˜„ì¬ ë³´ì´ëŠ” ë¬¼ì²´ëŠ” {seen}ì…ë‹ˆë‹¤."
        )
        if current_tts_thread and current_tts_thread.is_alive():
            current_tts_thread.join()
        answer = get_yes_no_response()
        if not answer:
            speak_feedback("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ê² ìŠµë‹ˆë‹¤.")
            sys.exit(0)
        else:
            speak_feedback("í•„ìš”í•œ ë¬¼ê±´ì„ ë§ì”€í•´ì£¼ì„¸ìš”.")
            get_command()
            # ìƒˆ ëª…ë ¹ì–´ ë¡œë“œ & GPT ì¶”ì¶œ
            cmd_text = load_command()
            info = extract_target_with_gpt(cmd_text)
            target = info.get("target")
            destination = info.get("destination")
            # ì¬ì´ˆê¸°í™” í›„ ê³„ì† ì§„í–‰

    # ì •ìƒ ì•ˆë‚´ ë¶„ê¸° (destination ìœ ë¬´ì— ë”°ë¼ ë‹¤ë¥¸ ì•ˆë‚´)
    seen = ", ".join(sorted(set(labels))) or "ì—†ìŒ"
    if destination:
        if tgt_pos and dst_pos:
            speak_feedback("íƒ€ê²Ÿê³¼ ëª©ì ì§€ ë¬¼ì²´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì•ˆë‚´ë¥¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.")
        elif tgt_pos:
            speak_feedback(f"{target}ë§Œ ì°¾ì•˜ìŠµë‹ˆë‹¤. í˜„ì¬ ë³´ì´ëŠ” ë¬¼ì²´ëŠ” {seen}ì…ë‹ˆë‹¤.")
            if current_tts_thread and current_tts_thread.is_alive():
                current_tts_thread.join()
            answer = get_yes_no_response()
            if not answer:
                speak_feedback("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•˜ê² ìŠµë‹ˆë‹¤.")
                sys.exit(0)
            else:
                speak_feedback("ì°¾ìœ¼ì‹¤ ë¬¼ê±´ì„ ë§ì”€í•´ì£¼ì„¸ìš”.")
                get_command()
                # ìƒˆ ëª…ë ¹ì–´ ë¡œë“œ & GPT ì¶”ì¶œ
                cmd_text = load_command()
                info = extract_target_with_gpt(cmd_text)
                target = info.get("target")
                destination = info.get("destination")
        else:
            speak_feedback(f"{destination}ë§Œ ì°¾ì•˜ìŠµë‹ˆë‹¤. í˜„ì¬ ë³´ì´ëŠ” ë¬¼ì²´ëŠ” {seen}ì…ë‹ˆë‹¤.")
            if current_tts_thread and current_tts_thread.is_alive():
                current_tts_thread.join()
            answer = get_yes_no_response()
            if not answer:
                speak_feedback("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                sys.exit(0)
            else:
                speak_feedback("ì°¾ìœ¼ì‹¤ ë¬¼ê±´ì„ ë§ì”€í•´ì£¼ì„¸ìš”.")
                get_command()
                # ìƒˆ ëª…ë ¹ì–´ ë¡œë“œ & GPT ì¶”ì¶œ
                cmd_text = load_command()
                info = extract_target_with_gpt(cmd_text)
                target = info.get("target")
                destination = info.get("destination")
    else:
        if not tgt_pos:
            speak_feedback("ë¬¼ì²´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í˜„ì¬ ë³´ì´ëŠ” ë¬¼ì²´ëŠ” {seen}ì…ë‹ˆë‹¤. ")
            if current_tts_thread and current_tts_thread.is_alive():
                current_tts_thread.join()
            answer = get_yes_no_response()
            if not answer:
                speak_feedback("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                sys.exit(0)
            else:
                speak_feedback("ì°¾ìœ¼ì‹¤ ë¬¼ê±´ì„ ë§ì”€í•´ì£¼ì„¸ìš”.")
                get_command()
                # ìƒˆ ëª…ë ¹ì–´ ë¡œë“œ & GPT ì¶”ì¶œ
                cmd_text = load_command()
                info = extract_target_with_gpt(cmd_text)
                target = info.get("target")
                destination = info.get("destination")


    # íŒŒë…¸ë¼ë§ˆ ìœ„ì¹˜ ê²°ê³¼ë¡œ ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
    target_pos = last_seen_target_pos = tgt_pos
    destination_pos = last_seen_destination_pos = dst_pos
    pano_width, pano_height = pano.shape[1], pano.shape[0]
    pan_target_pos, pan_dest_pos = tgt_pos, dst_pos

    cap = cv2.VideoCapture(2, cv2.CAP_DSHOW)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    threading.Thread(target=feedback_loop, daemon=True).start()
    threading.Thread(target=yolo_loop, daemon=True).start()

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        now = time.time()
        with frame_lock:
            if target_pos and now - last_seen_target_time > EXPIRE_TIME:
                target_pos = None
                last_seen_target_pos = None
            if destination_pos and now - last_seen_destination_time > EXPIRE_TIME:
                destination_pos = None
                last_seen_destination_pos = None

            frame_for_display = frame.copy()
            if target_pos or destination_pos:
                hand_pos = detect_hand(frame)
            else:
                hand_pos = None

        if hand_pos:
            cv2.circle(frame_for_display, hand_pos, 10, (0, 255, 0), -1)

        now = time.time()
        tp = None
        if target_pos:
            tp = target_pos
        elif last_seen_target_pos and now - last_seen_target_time < EXPIRE_TIME:
            tp = last_seen_target_pos
        elif last_seen_target_pos and hand_pos:  # ì†ì´ ê°€ë ¤ì„œ ì•ˆ ë³´ì¸ ê²½ìš°: ì†ê³¼ ë§ˆì§€ë§‰ ìœ„ì¹˜ê°€ ê°€ê¹Œìš°ë©´ ìœ ì§€
            if math.hypot(hand_pos[0] - last_seen_target_pos[0],
                          hand_pos[1] - last_seen_target_pos[1]) < ARRIVE_THRESHOLD:
                tp = last_seen_target_pos

        if tp and isinstance(tp, tuple):
            cv2.circle(frame_for_display, tp, 10, (0, 0, 255), -1)

        dp = None
        if destination_pos:
            dp = destination_pos
        elif last_seen_destination_pos and now - last_seen_destination_time < EXPIRE_TIME:
            dp = last_seen_destination_pos
        elif last_seen_destination_pos and hand_pos:
            if math.hypot(hand_pos[0] - last_seen_destination_pos[0],
                          hand_pos[1] - last_seen_destination_pos[1]) < ARRIVE_THRESHOLD:
                dp = last_seen_destination_pos

        if dp and isinstance(dp, tuple):
            cv2.circle(frame_for_display, dp, 10, (255, 0, 0), -1)

        cv2.imshow("ì›¹ìº  ë¯¸ë¦¬ë³´ê¸°", frame_for_display)
        if cv2.waitKey(1) & 0xFF == 27:
            break

    cap.release()
    cv2.destroyAllWindows()
