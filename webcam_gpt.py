import os
import sys
import cv2
import json
import math
import time
import threading
import pyttsx3
import mediapipe as mp
from ultralytics import YOLO
from datetime import datetime
import openai
import base64

# ì„¤ì •
TASK_FILENAME = "task_plan_destination.json"
ARRIVE_THRESHOLD = 50
NEAR_THRESHOLD = 150
DISTANCE_DELTA = 30
FEEDBACK_INTERVAL = 2.0
YOLO_INTERVAL = 1.5
TTS_RATE = 220
MIN_FEEDBACK_INTERVAL = 5.0
HAND_FEEDBACK_INTERVAL = 7.0
api_key = os.getenv("OPENAI_API_KEY")

# ì´ˆê¸°í™”
model = YOLO("yolov8n.pt")
tts = pyttsx3.init()
tts.setProperty("rate", TTS_RATE)
tts_lock = threading.Lock()
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1,
                       min_detection_confidence=0.7, min_tracking_confidence=0.7)

# ì „ì—­ ë³€ìˆ˜
target, destination = None, None
target_pos, destination_pos = None, None
last_seen_target_pos, last_seen_destination_pos = None, None
hand_pos, frame_for_display = None, None
frame_lock = threading.Lock()
last_hand_feedback_time = 0
near_intro_done = False

# í”¼ë“œë°± ìƒíƒœ
last_feedback_time, last_feedback_text = 0, None
current_tts_thread = None
step = "find_target"
prev_distance = None
target_intro_done, destination_intro_done = False, False
target_grabbed = False
last_close_to_target_time = None
initial_target_direction_given = False

def speak_feedback(text):
    global last_feedback_time, last_feedback_text, current_tts_thread
    now = time.time()
    if text == last_feedback_text and now - last_feedback_time < MIN_FEEDBACK_INTERVAL:
        return
    last_feedback_text = text
    last_feedback_time = now
    print("ğŸ—£", text)

    def tts_job():
        with tts_lock:
            try:
                tts.stop()
                tts.say(text)
                tts.runAndWait()
            except:
                pass

    if current_tts_thread and current_tts_thread.is_alive():
        pass
    current_tts_thread = threading.Thread(target=tts_job, daemon=True)
    current_tts_thread.start()

def speak_hand_feedback(text):
    global last_hand_feedback_time, current_tts_thread
    now = time.time()
    if now - last_hand_feedback_time < HAND_FEEDBACK_INTERVAL:
        return
    last_hand_feedback_time = now
    print("ğŸ—£", text)

    def tts_job():
        with tts_lock:
            try:
                tts.stop()
                tts.say(text)
                tts.runAndWait()
            except:
                pass

    if current_tts_thread and current_tts_thread.is_alive():
        pass
    current_tts_thread = threading.Thread(target=tts_job, daemon=True)
    current_tts_thread.start()

def detect_hand(image):
    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb)
    if results.multi_hand_landmarks:
        h, w, _ = image.shape
        palm = [results.multi_hand_landmarks[0].landmark[i] for i in [0, 1, 5, 9, 13, 17]]
        x = int(sum(p.x for p in palm) / len(palm) * w)
        y = int(sum(p.y for p in palm) / len(palm) * h)
        return (x, y)
    return None

def find_object_position(image, label, min_conf=0.6):
    resized = cv2.resize(image, (320, 320))
    results = model(resized, verbose=False)
    scale_x = image.shape[1] / 320
    scale_y = image.shape[0] / 320
    for result in results:
        for box in result.boxes:
            cls = model.names[int(box.cls)].lower()
            conf = float(box.conf[0])
            if label.lower() in cls and conf > min_conf:
                x1, y1, x2, y2 = box.xyxy[0].tolist()
                cx = int((x1 + x2) / 2 * scale_x)
                cy = int(y2 * scale_y)  # í•˜ë‹¨ ë¶€ë¶„
                return (cx, cy)
    return None

def get_initial_direction_comment(pos, frame_size):
    if pos is None:
        return "íƒ€ê²Ÿ ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    x, y = pos
    w, h = frame_size
    x_rel, y_rel = x / w, y / h
    dir_x = "ì™¼ìª½" if x_rel < 0.3 else "ì˜¤ë¥¸ìª½" if x_rel > 0.7 else "ê°€ìš´ë°"
    dir_y = "ìœ„" if y_rel < 0.3 else "ì•„ë˜" if y_rel > 0.7 else "ê°€ìš´ë°"
    if dir_x == "ê°€ìš´ë°" and dir_y == "ê°€ìš´ë°":
        return "íƒ€ê²Ÿì´ ì •ì¤‘ì•™ì— ìˆìŠµë‹ˆë‹¤."
    else:
        return f"íƒ€ê²Ÿì´ {dir_x} {dir_y}ì— ìˆì–´ìš”."

def ask_gpt_if_grabbed(image, target):
    question = (
        f"ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì‚¬ëŒì´ '{target}'ë¥¼ ì†ìœ¼ë¡œ í™•ì‹¤íˆ ì¡ê³  ìˆëŠ”ì§€ íŒë‹¨í•´ì¤˜. "
        "ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì—ë§Œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´:\n"
        '{ "grabbed": "true" }\n\n'
        "- ì†ì´ ë¬¼ì²´ë¥¼ ê°ì‹¸ê³  ìˆê±°ë‚˜\n"
        "- ì†ê°€ë½ì´ ë¬¼ì²´ì˜ ì–‘ìª½ì„ ì¡ê³  ìˆìœ¼ë©°\n"
        "- ì†ì´ ë¬¼ì²´ ìœ„ì— ë†“ì—¬ í™•ì‹¤íˆ ê³ ì •ëœ ìƒíƒœë¼ë©´ true.\n\n"
        "ì†ì´ ê·¼ì²˜ì— ìˆê±°ë‚˜ ì†ê°€ë½ì´ ë¬¼ì²´ ìœ„ì— ì—†ìœ¼ë©´ falseë¡œ íŒë‹¨í•´ì¤˜:\n"
        '{ "grabbed": "false" }\n\n'
        "ë°˜ë“œì‹œ ìœ„ JSON í˜•ì‹ ì¤‘ í•˜ë‚˜ë¡œë§Œ ëŒ€ë‹µí•´ì¤˜."
    )

    _, buffer = cv2.imencode('.jpg', image)
    img_bytes = buffer.tobytes()
    base64_img = base64.b64encode(img_bytes).decode("utf-8")

    print("ğŸ§  GPT íŒë‹¨ ìš”ì²­ ì¤‘...")

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": question},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{base64_img}"}
                        }
                    ]
                }
            ],
            max_tokens=300,
        )

        result = response['choices'][0]['message']['content']
        print("ğŸ§  GPT ì‘ë‹µ:", result)

        # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„
        try:
            parsed = json.loads(result)
            return str(parsed.get("grabbed", "")).strip().lower() == "true"
        except json.JSONDecodeError:
            print("âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨. ì‘ë‹µ ë‚´ìš© í™•ì¸:", result)
            return False

    except Exception as e:
        print("[GPT ì˜¤ë¥˜]", e)
        return False


# ------------------ í”¼ë“œë°± ë£¨í”„ ------------------

def feedback_loop():
    global step, target_intro_done, destination_intro_done
    global prev_distance, target_grabbed, last_close_to_target_time
    global initial_target_direction_given, last_hand_feedback_time
    global near_intro_done

    while True:
        time.sleep(FEEDBACK_INTERVAL)

        with frame_lock:
            hand = hand_pos
            target = target_pos or last_seen_target_pos
            dest = destination_pos or last_seen_destination_pos
            frame = frame_for_display.copy() if frame_for_display is not None else None

        if step == "find_target":
            if frame is None:
                continue
            if not target and not destination:
                speak_feedback("íƒ€ê²Ÿê³¼ ëª©ì ì§€ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue
            elif not target:
                speak_feedback("íƒ€ê²Ÿì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue
            elif not destination:
                speak_feedback("ëª©ì ì§€ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue

            if not initial_target_direction_given and target and isinstance(target, tuple):
                msg = get_initial_direction_comment(target, (frame.shape[1], frame.shape[0]))
                speak_feedback(msg)
                initial_target_direction_given = True

            if not hand:
                now = time.time()
                if now - last_hand_feedback_time > HAND_FEEDBACK_INTERVAL:
                    speak_hand_feedback("ì†ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    last_hand_feedback_time = now
                continue

            hx, hy = hand
            tx, ty = target
            dx, dy = tx - hx, ty - hy
            distance = math.sqrt(dx ** 2 + dy ** 2)

            if distance < ARRIVE_THRESHOLD:
                if not last_close_to_target_time:
                    last_close_to_target_time = datetime.now()
                elif (datetime.now() - last_close_to_target_time).total_seconds() >= 3:
                    speak_feedback("ì†ì„ ë»—ì–´ ì¡ìœ¼ì„¸ìš”.")

                    # 3ì´ˆ ëŒ€ê¸° í›„ í”„ë ˆì„ ìº¡ì²˜
                    time.sleep(3.0)

                    with frame_lock:
                        latest_frame = frame_for_display.copy() if frame_for_display is not None else None

                    if latest_frame is not None and hand is not None:
                        hx, hy = hand
                        tx, ty = target
                        h, w, _ = latest_frame.shape
                        mx, my = int((hx + tx) / 2), int((hy + ty) / 2)
                        x1, x2 = max(0, mx - 200), min(w, mx + 200)
                        y1, y2 = max(0, my - 200), min(h, my + 200)
                        crop = latest_frame[y1:y2, x1:x2]
                        cv2.imwrite("debug_crop.jpg", crop)

                        # speak_feedback("ì†ì´ ë¬¼ì²´ë¥¼ ì¡ì•˜ëŠ”ì§€ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤.")
                        is_grabbed = ask_gpt_if_grabbed(crop, target)

                        if isinstance(is_grabbed, str):
                            is_grabbed = is_grabbed.strip().lower() == "true"

                        if is_grabbed:
                            #speak_feedback("ì†ì´ ë¬¼ì²´ë¥¼ ì¡ì€ ê²ƒì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            target_grabbed = True
                            step = "move_to_destination"
                            destination_intro_done = False
                            target_intro_done = False
                            time.sleep(1.5)
                            continue
                        else:
                            speak_feedback("ì•„ì§ ì¡ì§€ ì•Šì€ ê²ƒ ê°™ì•„ìš”.")
            else:
                    last_close_to_target_time = None

            direction = "ì˜¤ë¥¸ìª½" if dx > 0 else "ì™¼ìª½" if abs(dx) > abs(dy) else "ì•„ë˜" if dy > 0 else "ìœ„"
            speak_feedback(f"{direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")

            # if distance < NEAR_THRESHOLD:
            #     if not near_intro_done:
            #         speak_feedback(f"ê±°ì˜ ë„ì°©í–ˆì–´ìš”. {direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
            #         near_intro_done = True
            #     else:
            #         speak_feedback(f"{direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
            # else:
            #     near_intro_done = False
            #     if not target_intro_done:
            #         speak_feedback(f"íƒ€ê²Ÿì— ì ‘ê·¼ ì¤‘ì…ë‹ˆë‹¤. {direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
            #         target_intro_done = True
            #     else:
            #         speak_feedback(f"{direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
            prev_distance = distance

        elif step == "move_to_destination":
            if frame is None:
                continue
            if not dest:
                speak_feedback("ëª©ì ì§€ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue
            if not hand:
                now = time.time()
                if now - last_hand_feedback_time > HAND_FEEDBACK_INTERVAL:
                    speak_hand_feedback("ì†ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    last_hand_feedback_time = now
                continue

            hx, hy = hand
            dx, dy = dest[0] - hx, dest[1] - hy
            distance = math.sqrt(dx ** 2 + dy ** 2)

            if distance < ARRIVE_THRESHOLD:
                speak_feedback("ëª©ì ì§€ì— ë„ì°©í–ˆìŠµë‹ˆë‹¤. ë‚´ë ¤ë†“ìœ¼ì„¸ìš”.")
                step = "done"
                continue

            direction = "ì˜¤ë¥¸ìª½" if dx > 0 else "ì™¼ìª½" if abs(dx) > abs(dy) else "ì•„ë˜" if dy > 0 else "ìœ„"

            if not destination_intro_done:
                speak_feedback("ëª©ì ì§€ë¡œ ì´ë™ ì¤‘ì…ë‹ˆë‹¤.")
                destination_intro_done = True
            else:
                speak_feedback(f"{direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")


# ------------------ ê°ì²´ ìœ„ì¹˜ ê°±ì‹  ë£¨í”„ ------------------

def yolo_loop():
    global target_pos, destination_pos, last_seen_target_pos, last_seen_destination_pos
    while True:
        time.sleep(YOLO_INTERVAL)
        with frame_lock:
            frame = frame_for_display.copy() if frame_for_display is not None else None
        if frame is None:
            continue

        if target:
            pos = find_object_position(frame, target)
            if pos:
                target_pos = pos
                last_seen_target_pos = pos
        if destination:
            pos = find_object_position(frame, destination)
            if pos:
                destination_pos = pos
                last_seen_destination_pos = pos


# ------------------ ë©”ì¸ ------------------

def load_target_info():
    try:
        with open(TASK_FILENAME, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get("target"), data.get("destination")
    except:
        return None, None

if __name__ == "__main__":
    target, destination = load_target_info()
    if not target or not destination:
        print("âŒ íƒ€ê²Ÿ ë˜ëŠ” ëª©ì ì§€ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        exit()
    print(f"ğŸ“Œ íƒ€ê²Ÿ: {target}, ëª©ì ì§€: {destination}")

    cap = cv2.VideoCapture(2, cv2.CAP_DSHOW)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    threading.Thread(target=feedback_loop, daemon=True).start()
    threading.Thread(target=yolo_loop, daemon=True).start()

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        with frame_lock:
            frame_for_display = frame.copy()
            if any([target_pos, destination_pos, last_seen_target_pos, last_seen_destination_pos]):
                hand_pos = detect_hand(frame)
            else:
                hand_pos = None

        if hand_pos:
            cv2.circle(frame_for_display, hand_pos, 10, (0, 255, 0), -1)
        tp = target_pos or last_seen_target_pos
        if tp and isinstance(tp, tuple):
            cv2.circle(frame_for_display, tp, 10, (0, 0, 255), -1)
        dp = destination_pos or last_seen_destination_pos
        if dp and isinstance(dp, tuple):
            cv2.circle(frame_for_display, dp, 10, (255, 0, 0), -1)

        cv2.imshow("ì›¹ìº  ë¯¸ë¦¬ë³´ê¸°", frame_for_display)
        if cv2.waitKey(1) & 0xFF == 27:
            break

    cap.release()
    cv2.destroyAllWindows()
