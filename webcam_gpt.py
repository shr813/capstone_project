import os
import sys
import cv2
import json
import math
import time
import threading
import pyttsx3
import mediapipe as mp
from ultralytics import YOLO
from datetime import datetime
import openai
import base64

# ì„¤ì •
TASK_FILENAME = "task_plan_destination.json"
ARRIVE_THRESHOLD = 50
NEAR_THRESHOLD = 150
DISTANCE_DELTA = 30
FEEDBACK_INTERVAL = 1.5
YOLO_INTERVAL = 1.5
TTS_RATE = 200
MIN_FEEDBACK_INTERVAL = 4
GRAB_HOLD_DURATION = 3.0  # 3ì´ˆ ì´ìƒ ê°€ê¹Œì´ ìˆìœ¼ë©´ ì¡ì€ ê²ƒìœ¼ë¡œ íŒë‹¨
HAND_FEEDBACK_INTERVAL = 9.0
api_key = os.getenv("OPENAI_API_KEY")

# ì´ˆê¸°í™”
model = YOLO("yolov8n.pt")
tts = pyttsx3.init()
tts.setProperty("rate", TTS_RATE)
tts_lock = threading.Lock()
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1,
                       min_detection_confidence=0.7, min_tracking_confidence=0.7)

# ì „ì—­ ìƒíƒœ ë³€ìˆ˜
target, destination = None, None
target_pos, destination_pos = None, None
last_seen_target_pos, last_seen_destination_pos = None, None
hand_pos, frame_for_display = None, None
frame_lock = threading.Lock()
last_hand_feedback_time = 0
near_intro_done = False

# í”¼ë“œë°± ìƒíƒœ
last_feedback_time, last_feedback_text = 0, None
current_tts_thread = None
step = "find_target"
prev_distance = None
target_intro_done, destination_intro_done = False, False
target_grabbed = False
last_close_to_target_time = None
initial_target_direction_given = False

# ------------------ ê¸°ëŠ¥ í•¨ìˆ˜ ------------------

def speak_feedback(text):
    global last_feedback_time, last_feedback_text, current_tts_thread
    now = time.time()
    if text == last_feedback_text and now - last_feedback_time < MIN_FEEDBACK_INTERVAL:
        return
    last_feedback_text = text
    last_feedback_time = now
    print("ğŸ—£", text)

    def tts_job():
        with tts_lock:
            try:
                tts.stop()
                tts.say(text)
                tts.runAndWait()
            except:
                pass

    if current_tts_thread and current_tts_thread.is_alive():
        pass
    current_tts_thread = threading.Thread(target=tts_job, daemon=True)
    current_tts_thread.start()

def speak_hand_feedback(text):
    global last_hand_feedback_time, current_tts_thread
    now = time.time()
    if now - last_hand_feedback_time < HAND_FEEDBACK_INTERVAL:
        return
    last_hand_feedback_time = now
    print("ğŸ—£", text)

    def tts_job():
        with tts_lock:
            try:
                tts.stop()
                tts.say(text)
                tts.runAndWait()
            except:
                pass

    if current_tts_thread and current_tts_thread.is_alive():
        pass
    current_tts_thread = threading.Thread(target=tts_job, daemon=True)
    current_tts_thread.start()


def detect_hand(image):
    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb)
    if results.multi_hand_landmarks:
        h, w, _ = image.shape
        palm = [results.multi_hand_landmarks[0].landmark[i] for i in [0, 1, 5, 9, 13, 17]]
        x = int(sum(p.x for p in palm) / len(palm) * w)
        y = int(sum(p.y for p in palm) / len(palm) * h)
        return (x, y)
    return None

def find_object_position(image, label, min_conf=0.6):
    resized = cv2.resize(image, (320, 320))
    results = model(resized, verbose=False)
    scale_x = image.shape[1] / 320
    scale_y = image.shape[0] / 320
    for result in results:
        for box in result.boxes:
            cls = model.names[int(box.cls)].lower()
            conf = float(box.conf[0])
            if label.lower() in cls and conf > min_conf:
                x1, y1, x2, y2 = box.xyxy[0].tolist()
                cx = int((x1 + x2) / 2 * scale_x)
                cy = int((y1 + y2) / 2 * scale_y)
                return (cx, cy)
    return None

def get_initial_direction_comment(pos, frame_size):
    x, y = pos
    w, h = frame_size
    x_rel, y_rel = x / w, y / h
    dir_x = "ì™¼ìª½" if x_rel < 0.3 else "ì˜¤ë¥¸ìª½" if x_rel > 0.7 else "ê°€ìš´ë°"
    dir_y = "ìœ„" if y_rel < 0.3 else "ì•„ë˜" if y_rel > 0.7 else "ê°€ìš´ë°"
    if dir_x == "ê°€ìš´ë°" and dir_y == "ê°€ìš´ë°":
        return "íƒ€ê²Ÿì´ ì •ì¤‘ì•™ì— ìˆìŠµë‹ˆë‹¤."
    else:
        return f"íƒ€ê²Ÿì´ {dir_x} {dir_y}ì— ìˆì–´ìš”."

def ask_gpt_if_grabbed(image, target):
    question = (
        f"ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì‚¬ëŒì´ '{target}'ì„ ì†ìœ¼ë¡œ ì¡ê³  ìˆëŠ”ì§€ íŒë‹¨í•´ì¤˜. "
        f"ì†ì´ '{target}' ìœ„ì— ì™„ì „íˆ ì˜¬ë¼ê°€ ìˆê³  ì¼ë¶€ ê°ì‹¸ê³  ìˆê±°ë‚˜, í™•ì‹¤íˆ ì¡ê³  ìˆë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µí•´:\n"
        '{ "grabbed": "true" }\n\n'
        "ì†ì´ ê·¼ì²˜ì— ìˆì§€ë§Œ ëª…í™•í•˜ê²Œ ì¡ì§€ ì•Šì•˜ìœ¼ë©´:\n"
        '{ "grabbed": "false" }\n\n'
        f"ì¤‘ìš”: ì´ë¯¸ì§€ê°€ ë¶ˆë¶„ëª…í•˜ë”ë¼ë„ ì†ì´ '{target}'ì„ ê°ì‹¸ê³  ìˆê±°ë‚˜, '{target}' ëŒ€ë¶€ë¶„ì„ ë®ê³  ìˆë‹¤ë©´ trueë¡œ íŒë‹¨í•´ì¤˜."
    )

    _, buffer = cv2.imencode('.jpg', image)
    img_bytes = buffer.tobytes()
    base64_img = base64.b64encode(img_bytes).decode("utf-8")

    print("ğŸ§  GPT íŒë‹¨ ìš”ì²­ ì¤‘...")

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": question},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{base64_img}"}
                        }
                    ]
                }
            ],
            max_tokens=300,
        )

        result = response['choices'][0]['message']['content']
        print("ğŸ§  GPT ì‘ë‹µ:", result)

        # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„
        try:
            parsed = json.loads(result)
            return str(parsed.get("grabbed", "")).strip().lower() == "true"
        except json.JSONDecodeError:
            print("âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨. ì‘ë‹µ ë‚´ìš© í™•ì¸:", result)
            return False

    except Exception as e:
        print("[GPT ì˜¤ë¥˜]", e)
        return False


# ------------------ í”¼ë“œë°± ë£¨í”„ ------------------

def feedback_loop():
    global step, target_intro_done, destination_intro_done
    global prev_distance, target_grabbed, last_close_to_target_time
    global initial_target_direction_given, last_hand_feedback_time
    global near_intro_done

    horizontal_aligned = False

    while True:
        time.sleep(FEEDBACK_INTERVAL)

        with frame_lock:
            hand = hand_pos
            target = target_pos or last_seen_target_pos
            dest = destination_pos or last_seen_destination_pos
            frame = frame_for_display.copy() if frame_for_display is not None else None

        if step == "find_target":
            if frame is None:
                continue

            if not target and not destination:
                speak_feedback("íƒ€ê²Ÿê³¼ ëª©ì ì§€ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue
            elif not target:
                speak_feedback("íƒ€ê²Ÿì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue
            elif not destination:
                speak_feedback("ëª©ì ì§€ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue

            if not initial_target_direction_given and target:
                msg = get_initial_direction_comment(target, (frame.shape[1], frame.shape[0]))
                speak_feedback(msg)
                initial_target_direction_given = True

            if not hand:
                now = time.time()
                if now - last_hand_feedback_time > HAND_FEEDBACK_INTERVAL:
                    speak_hand_feedback("ì†ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    last_hand_feedback_time = now
                continue

            hx, hy = hand
            tx, ty = target
            dx, dy = tx - hx, ty - hy
            distance = math.sqrt(dx ** 2 + dy ** 2)

            if distance < ARRIVE_THRESHOLD:
                if not last_close_to_target_time:
                    last_close_to_target_time = datetime.now()
                elif (datetime.now() - last_close_to_target_time).total_seconds() >= GRAB_HOLD_DURATION:
                    speak_feedback("ë„ì°©í–ˆìŠµë‹ˆë‹¤. ì†ì„ ë»—ì–´ ì¡ìœ¼ì„¸ìš”.")

                    # 3ì´ˆ ëŒ€ê¸° í›„ í”„ë ˆì„ ìº¡ì²˜
                    time.sleep(3.0)

                    if frame is not None:
                        speak_feedback("ì†ì´ ë¬¼ì²´ë¥¼ ì¡ì•˜ëŠ”ì§€ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤.")
                        is_grabbed = ask_gpt_if_grabbed(frame, target)

                        if isinstance(is_grabbed, str):
                            is_grabbed = is_grabbed.strip().lower() == "true"

                        if is_grabbed:
                            speak_feedback("ì†ì´ ë¬¼ì²´ë¥¼ ì¡ì€ ê²ƒì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            target_grabbed = True
                            step = "move_to_destination"
                            destination_intro_done = False
                            target_intro_done = False
                            time.sleep(1.5)
                            continue
                        else:
                            speak_feedback("ì•„ì§ ì¡ì§€ ì•Šì€ ê²ƒ ê°™ì•„ìš”. ë” ê°€ê¹Œì´ ì ‘ê·¼í•˜ì„¸ìš”.")
            else:
                last_close_to_target_time = None

            direction = (
                "ì˜¤ë¥¸ìª½" if dx > 0 else "ì™¼ìª½"
            ) if abs(dx) > abs(dy) else (
                "ì•„ë˜" if dy > 0 else "ìœ„"
            )

            if distance < NEAR_THRESHOLD:
                if not near_intro_done:
                    speak_feedback(f"ê±°ì˜ ë„ì°©í–ˆì–´ìš”. {direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
                    near_intro_done = True
                else:
                    speak_feedback(f"{direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
            else:
                near_intro_done = False
                if not target_intro_done:
                    speak_feedback(f"íƒ€ê²Ÿì— ì ‘ê·¼ ì¤‘ì…ë‹ˆë‹¤. {direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
                    target_intro_done = True
                else:
                    speak_feedback(f"{direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")
            prev_distance = distance

        elif step == "move_to_destination":
            if frame is None:
                continue

            if not dest:
                speak_feedback("ëª©ì ì§€ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue

            if not hand:
                now = time.time()
                if now - last_hand_feedback_time > HAND_FEEDBACK_INTERVAL:
                    speak_hand_feedback("ì†ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    last_hand_feedback_time = now
                continue

            hx, hy = hand
            dx, dy = dest[0] - hx, dest[1] - hy
            distance = math.sqrt(dx ** 2 + dy ** 2)

            if distance < ARRIVE_THRESHOLD:
                speak_feedback("ëª©ì ì§€ì— ë„ì°©í–ˆìŠµë‹ˆë‹¤. ë‚´ë ¤ë†“ìœ¼ì„¸ìš”.")
                step = "done"
                continue

            direction = (
                "ì˜¤ë¥¸ìª½" if dx > 0 else "ì™¼ìª½"
            ) if abs(dx) > abs(dy) else (
                "ì•„ë˜" if dy > 0 else "ìœ„"
            )

            if not destination_intro_done:
                speak_feedback("ëª©ì ì§€ë¡œ ì´ë™ ì¤‘ì…ë‹ˆë‹¤.")
                destination_intro_done = True
            else:
                speak_feedback(f"{direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.")

# ------------------ ê°ì²´ ìœ„ì¹˜ ê°±ì‹  ë£¨í”„ ------------------

def yolo_loop():
    global target_pos, destination_pos, last_seen_target_pos, last_seen_destination_pos
    while True:
        time.sleep(YOLO_INTERVAL)
        with frame_lock:
            frame = frame_for_display.copy() if frame_for_display is not None else None
        if frame is None:
            continue

        if target:
            pos = find_object_position(frame, target)
            if pos:
                target_pos = pos
                last_seen_target_pos = pos

        if destination:
            pos = find_object_position(frame, destination)
            if pos:
                destination_pos = pos
                last_seen_destination_pos = pos

# ------------------ ë©”ì¸ ------------------

def load_target_info():
    try:
        with open(TASK_FILENAME, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get("target"), data.get("destination")
    except:
        return None, None

if __name__ == "__main__":
    target, destination = load_target_info()
    if not target or not destination:
        print("âŒ íƒ€ê²Ÿ ë˜ëŠ” ëª©ì ì§€ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        exit()

    print(f"ğŸ“Œ íƒ€ê²Ÿ: {target}, ëª©ì ì§€: {destination}")

    cap = cv2.VideoCapture(2, cv2.CAP_DSHOW)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    threading.Thread(target=feedback_loop, daemon=True).start()
    threading.Thread(target=yolo_loop, daemon=True).start()

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        with frame_lock:
            frame_for_display = frame.copy()
            if target_pos or destination_pos or last_seen_target_pos or last_seen_destination_pos:
                hand_pos = detect_hand(frame)
            else:
                hand_pos = None

        if hand_pos:
            cv2.circle(frame_for_display, hand_pos, 10, (0, 255, 0), -1)
        if target_pos or last_seen_target_pos:
            cv2.circle(frame_for_display, target_pos or last_seen_target_pos, 10, (0, 0, 255), -1)
        if destination_pos or last_seen_destination_pos:
            cv2.circle(frame_for_display, destination_pos or last_seen_destination_pos, 10, (255, 0, 0), -1)

        cv2.imshow("ì›¹ìº  ë¯¸ë¦¬ë³´ê¸°", frame_for_display)
        if cv2.waitKey(1) & 0xFF == 27:
            break

    cap.release()
    cv2.destroyAllWindows()
