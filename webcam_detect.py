import os
import sys
import cv2
import json
import math
import time
import threading
import subprocess
import pyttsx3
import mediapipe as mp
from ultralytics import YOLO

# ì„¤ì •
TASK_FILENAME = "task_plan.json"
ARRIVE_THRESHOLD = 50
NEAR_THRESHOLD = 150
DISTANCE_DELTA = 30
FEEDBACK_INTERVAL = 0.7
YOLO_INTERVAL = 1.5

# ì´ˆê¸°í™”
model = YOLO("yolov8n.pt")
tts = pyttsx3.init()
tts.setProperty("rate", 200)
tts_lock = threading.Lock()
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1)

# ìƒíƒœ ë³€ìˆ˜
target_english = None
last_known_target_pos = None
prev_distance = None
hand_pos = None
frame_for_display = None
found_target_recently = False
initial_guided = False
miss_count = 0
frame_lock = threading.Lock()

# í”¼ë“œë°± ì¤‘ë³µ ë°©ì§€
last_feedback_text = None
last_feedback_time = 0
MIN_FEEDBACK_INTERVAL = 3.5


def give_feedback(text):
    global last_feedback_text, last_feedback_time

    now = time.time()
    if text == last_feedback_text and now - last_feedback_time < MIN_FEEDBACK_INTERVAL:
        return

    print(f"ğŸ—£ {text}")
    last_feedback_text = text
    last_feedback_time = now

    def speak():
        with tts_lock:
            try:
                tts.say(text)
                tts.runAndWait()
            except:
                pass

    threading.Thread(target=speak, daemon=True).start()


def detect_hand(image):
    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb)
    if results.multi_hand_landmarks:
        h, w, _ = image.shape
        palm = [results.multi_hand_landmarks[0].landmark[i] for i in [0, 1, 5, 9, 13, 17]]
        x = int(sum(l.x for l in palm) / len(palm) * w)
        y = int(sum(l.y for l in palm) / len(palm) * h)
        return (x, y)
    return None


def find_target_position(image, label, min_conf=0.5):
    resized = cv2.resize(image, (320, 320))
    results = model(resized, verbose=False)
    scale_x = image.shape[1] / 320
    scale_y = image.shape[0] / 320

    for result in results:
        for box in result.boxes:
            conf = float(box.conf[0])
            cls = model.names[int(box.cls)].lower()
            if label.lower() in cls and conf > min_conf:
                x1, y1, x2, y2 = box.xyxy[0].tolist()
                cx = int((x1 + x2) / 2 * scale_x)
                cy = int((y1 + y2) / 2 * scale_y)
                return (cx, cy)
    return None



def get_initial_direction_comment(pos, frame_size):
    x, y = pos
    w, h = frame_size
    x_rel, y_rel = x / w, y / h

    dir_x = "ì™¼ìª½" if x_rel < 0.3 else "ì˜¤ë¥¸ìª½" if x_rel > 0.7 else "ê°€ìš´ë°"
    dir_y = "ìœ„" if y_rel < 0.3 else "ì•„ë˜" if y_rel > 0.7 else "ê°€ìš´ë°"

    if dir_x == "ê°€ìš´ë°" and dir_y == "ê°€ìš´ë°":
        return "íƒ€ê²Ÿì´ ì •ì¤‘ì•™ì— ìˆìŠµë‹ˆë‹¤."
    else:
        return f"íƒ€ê²Ÿì´ {dir_x} {dir_y}ì— ìˆì–´ìš”."


def load_target_name():
    try:
        with open(TASK_FILENAME, "r", encoding="utf-8") as f:
            return json.load(f).get("target_english", None)
    except:
        return None


def feedback_loop():
    global last_known_target_pos, prev_distance
    target_missing_warned = False
    last_hand_feedback_time = 0
    last_distance_feedback_time = 0  # NEW
    HAND_FEEDBACK_COOLDOWN = 10
    DISTANCE_FEEDBACK_COOLDOWN = 4.5  # NEW

    while True:
        time.sleep(FEEDBACK_INTERVAL)

        with frame_lock:
            hand = hand_pos
            target = last_known_target_pos

        if not target:
            if not target_missing_warned:
                give_feedback("íƒ€ê²Ÿì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì£¼ë³€ì„ ë‘˜ëŸ¬ë´ ì£¼ì„¸ìš”.")
                target_missing_warned = True
            continue
        else:
            target_missing_warned = False

        if not hand:
            now = time.time()
            if now - last_hand_feedback_time > HAND_FEEDBACK_COOLDOWN:
                give_feedback("ì†ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                last_hand_feedback_time = now
            continue

        hx, hy = hand
        tx, ty = target
        dx, dy = tx - hx, ty - hy
        distance = math.sqrt(dx**2 + dy**2)

        direction = "ì˜¤ë¥¸ìª½" if dx > 0 else "ì™¼ìª½" if abs(dx) > abs(dy) else "ì•„ë˜" if dy > 0 else "ìœ„"
        msg = None
        now = time.time()

        if distance < ARRIVE_THRESHOLD:
            msg = "ë„ì°©í–ˆìŠµë‹ˆë‹¤! ì†ì„ ë»—ì–´ ì¡ìœ¼ì„¸ìš”."
        elif distance < NEAR_THRESHOLD:
            msg = f"ê±°ì˜ ë„ì°©í–ˆì–´ìš”! {direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”."
        elif prev_distance is not None:
            delta = distance - prev_distance
            if delta < -DISTANCE_DELTA:
                msg = f"ì†ì´ ì˜ ì ‘ê·¼í•˜ê³  ìˆì–´ìš”. {direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”."
            elif delta > DISTANCE_DELTA:
                msg = f"ì†ì´ ë©€ì–´ì§€ê³  ìˆì–´ìš”. {direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”."
            else:
                msg = f"ê±°ë¦¬ë¥¼ ìœ ì§€í•˜ê³  ìˆì–´ìš”. {direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”."
        else:
            msg = f"ì† ìœ„ì¹˜ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤. {direction}ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”."

        prev_distance = distance

        if msg and now - last_distance_feedback_time > DISTANCE_FEEDBACK_COOLDOWN:
            give_feedback(msg)
            last_distance_feedback_time = now



def yolo_loop():
    global last_known_target_pos, found_target_recently, initial_guided, miss_count, target_english

    while True:
        time.sleep(YOLO_INTERVAL)

        with frame_lock:
            if frame_for_display is not None:
                pos = find_target_position(frame_for_display.copy(), target_english)
                if pos:
                    last_known_target_pos = pos
                    found_target_recently = True
                    miss_count = 0
                    if not initial_guided:
                        comment = get_initial_direction_comment(
                            pos,
                            (frame_for_display.shape[1], frame_for_display.shape[0])
                        )
                        give_feedback(comment)
                        initial_guided = True
                else:
                    found_target_recently = False
                    miss_count += 1

                    if miss_count == 3:
                        give_feedback("íƒ€ê²Ÿì´ ë³´ì´ì§€ ì•Šì•„ìš”. ì¹´ë©”ë¼ë¥¼ ì¢Œìš°ë¡œ ì²œì²œíˆ ì›€ì§ì—¬ ì£¼ì„¸ìš”.")
                    elif miss_count == 6:
                        give_feedback("ì—¬ì „íˆ íƒ€ê²Ÿì´ ì—†ìŠµë‹ˆë‹¤. ìœ„ì•„ë˜ë¡œë„ ë¹„ì¶°ë´ ì£¼ì„¸ìš”.")
                    elif miss_count == 9:
                        give_feedback("ì¡°ëª…ì´ ì–´ë‘ìš¸ ìˆ˜ ìˆì–´ìš”. ì¡°ëª…ì„ ì¼œê±°ë‚˜ ë¬¼ëŸ¬ë‚˜ ë³´ì„¸ìš”.")
                    elif miss_count == 12:
                        give_feedback("íƒ€ê²Ÿì„ ì°¾ì§€ ëª»í–ˆì–´ìš”. ê°ë„ë¥¼ ë°”ê¿”ë³´ì„¸ìš”.")
                    elif miss_count >= 15:
                        give_feedback("íƒ€ê²Ÿ ì¸ì‹ì— ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤. ìŒì„± ëª…ë ¹ì„ ë‹¤ì‹œ ë§í•´ì£¼ì„¸ìš”.")
                        subprocess.run([sys.executable, "voice_command.py"])
                        subprocess.run([sys.executable, "task_planner.py"])
                        os.execv(sys.executable, [sys.executable] + sys.argv)


if __name__ == "__main__":
    target_english = load_target_name()
    if not target_english:
        print("âŒ íƒ€ê²Ÿ ì´ë¦„ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨")
        exit()

    print(f"ğŸ“Œ íƒ€ê²Ÿ ì´ë¦„: {target_english}")

    cap = cv2.VideoCapture(2)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    threading.Thread(target=feedback_loop, daemon=True).start()
    threading.Thread(target=yolo_loop, daemon=True).start()

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        with frame_lock:
            frame_for_display = frame.copy()

        with frame_lock:
            if found_target_recently:
                hand_pos = detect_hand(frame)
            else:
                hand_pos = None

        if hand_pos:
            cv2.circle(frame_for_display, hand_pos, 10, (0, 255, 0), -1)
        if last_known_target_pos:
            cv2.circle(frame_for_display, last_known_target_pos, 10, (0, 0, 255), -1)

        cv2.imshow("ì›¹ìº  ë¯¸ë¦¬ë³´ê¸°", frame_for_display)
        if cv2.waitKey(1) & 0xFF == 27:
            break

    cap.release()
    cv2.destroyAllWindows()
