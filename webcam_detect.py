import os
import cv2
import json
import math
import time
import threading
import pyttsx3
import mediapipe as mp
from ultralytics import YOLO

# ì„¤ì •
TASK_FILENAME = "task_plan.json"
ARRIVE_THRESHOLD = 50      # ë„ì°©ìœ¼ë¡œ ì¸ì‹í•  ê±°ë¦¬ (ì •ë°€í•˜ê²Œ)
NEAR_THRESHOLD = 150       # ê±°ì˜ ë„ì°©í•œ ê²½ìš°
DISTANCE_DELTA = 30        # ê±°ë¦¬ ë³€í™” ê°ì§€ ë¯¼ê°ë„
FEEDBACK_INTERVAL = 0.7
YOLO_INTERVAL = 1.5

# ì´ˆê¸°í™”
model = YOLO("yolov8n.pt")
tts = pyttsx3.init()
tts.setProperty("rate", 200)
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1)

# ìƒíƒœ ì €ì¥ìš© ë³€ìˆ˜
target_english = None
last_known_target_pos = None
prev_distance = None
last_feedback_time = 0
hand_pos = None
frame_for_display = None

frame_lock = threading.Lock()
feedback_lock = threading.Lock()

def give_feedback(text):
    print(f"ğŸ—£ {text}")
    try:
        tts.say(text)
        tts.runAndWait()
    except:
        pass
    finally:
        tts.stop()

def detect_hand(image):
    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb)
    if results.multi_hand_landmarks:
        h, w, _ = image.shape
        landmarks = results.multi_hand_landmarks[0].landmark
        palm_indices = [0, 1, 5, 9, 13, 17]
        avg_x = sum([landmarks[i].x for i in palm_indices]) / len(palm_indices)
        avg_y = sum([landmarks[i].y for i in palm_indices]) / len(palm_indices)
        return (int(avg_x * w), int(avg_y * h))
    return None

def find_target_position(image, label):
    resized = cv2.resize(image, (320, 320))
    results = model(resized, verbose=False)
    scale_x = image.shape[1] / 320
    scale_y = image.shape[0] / 320

    for result in results:
        for box in result.boxes:
            cls_name = model.names[int(box.cls)].lower()
            if label.lower() in cls_name:
                x1, y1, x2, y2 = box.xyxy[0].tolist()
                cx = int((x1 + x2) / 2 * scale_x)
                cy = int((y1 + y2) / 2 * scale_y)
                return (cx, cy)
    return None

def load_target_name():
    try:
        with open(TASK_FILENAME, "r", encoding="utf-8") as f:
            return json.load(f).get("target_english", None)
    except:
        return None

def feedback_loop():
    global last_known_target_pos, prev_distance, last_feedback_time

    while True:
        time.sleep(FEEDBACK_INTERVAL)

        with frame_lock:
            current_hand = hand_pos
            current_target = last_known_target_pos

        if not current_hand:
            give_feedback("ì†ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            continue

        if not current_target:
            give_feedback("íƒ€ê²Ÿì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            continue

        hx, hy = current_hand
        ox, oy = current_target
        dx, dy = ox - hx, oy - hy
        distance = math.sqrt(dx**2 + dy**2)

        if abs(dx) > abs(dy):
            direction = "ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”." if dx > 0 else "ì™¼ìª½ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”."
        else:
            direction = "ì•„ë˜ë¡œ ì´ë™í•˜ì„¸ìš”." if dy > 0 else "ìœ„ë¡œ ì´ë™í•˜ì„¸ìš”."

        if distance < ARRIVE_THRESHOLD:
            feedback = "ë„ì°©í–ˆìŠµë‹ˆë‹¤! ì†ì„ ë»—ì–´ ì¡ìœ¼ì„¸ìš”."
        elif distance < NEAR_THRESHOLD:
            feedback = f"ê±°ì˜ ë„ì°©í–ˆì–´ìš”! {direction}"
        elif prev_distance is not None:
            delta = distance - prev_distance
            if delta < -DISTANCE_DELTA:
                feedback = f"ì†ì´ ì˜ ì ‘ê·¼í•˜ê³  ìˆì–´ìš”. {direction}"
            elif delta > DISTANCE_DELTA:
                feedback = f"ì†ì´ ë©€ì–´ì§€ê³  ìˆì–´ìš”. {direction}"
            else:
                feedback = f"ê±°ë¦¬ë¥¼ ìœ ì§€í•˜ê³  ìˆì–´ìš”. {direction}"
        else:
            feedback = f"ì† ìœ„ì¹˜ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤. {direction}"

        prev_distance = distance
        give_feedback(feedback)

def yolo_loop():
    global last_known_target_pos

    while True:
        time.sleep(YOLO_INTERVAL)

        with frame_lock:
            if frame_for_display is not None:
                pos = find_target_position(frame_for_display, target_english)
                if pos:
                    last_known_target_pos = pos

if __name__ == "__main__":
    target_english= load_target_name()

    if not target_english:
        print("âŒ íƒ€ê²Ÿ ì´ë¦„ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨")
        exit()

    print(f" íƒ€ê²Ÿ ì´ë¦„: {target_english}")

    cap = cv2.VideoCapture(2)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    threading.Thread(target=feedback_loop, daemon=True).start()
    threading.Thread(target=yolo_loop, daemon=True).start()

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame = cv2.flip(frame, 1)
        hand = detect_hand(frame)

        with frame_lock:
            frame_for_display = frame.copy()
            hand_pos = hand

        if hand:
            cv2.circle(frame_for_display, hand, 10, (0, 255, 0), -1)
        if last_known_target_pos:
            cv2.circle(frame_for_display, last_known_target_pos, 10, (0, 0, 255), -1)

        cv2.imshow("ì›¹ìº  ë¯¸ë¦¬ë³´ê¸°", frame_for_display)
        if cv2.waitKey(1) & 0xFF == 27:
            break

    cap.release()
    cv2.destroyAllWindows()
